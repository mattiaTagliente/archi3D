## Role

Implement **Phase 2 — Batch Create**. Your task is to create a robust `archi3d batch create` that:

1. reads the canonical `tables/items.csv` (from Phase 1),
2. materializes a **per-run** job list (the **manifest**) for one or more algorithms, and
3. **initializes/updates** the SSOT **`tables/generations.csv`** with one row **per job** in **status=`enqueued`**, using atomic, locked upserts from Phase 0.

**Do not** modify other phases’ behavior beyond what’s explicitly specified here. Keep the change small, localized, idempotent, and backward-compatible. Phases 0 and 1 primitives and conventions are authoritative.  

---

## Objectives of Phase 2

* Provide a deterministic, auditable **job registry** (`generations.csv`) that ties each generated child artifact to its parent item and the exact inputs (images) and algorithm chosen.
* Produce a **per-run manifest** under `runs/<run_id>/manifest.csv` derived from `generations.csv` (not the other way around). `generations.csv` is the Single Source of Truth.
* Append a structured summary to `logs/batch_create.log`.

---

## Non-Goals (Out of Scope)

* No worker execution, no consolidate, no metrics computation (FScore/VFScore).
* No scanning of the dataset or JSON enrichment (already handled by Phase 1).
* No network calls or downloader logic.
* No changes to `tables/items.csv` schema.

---

## Repository Pointers (what you will modify)

* **CLI**: `src/archi3d/cli.py` — wire the `batch create` subcommand (args + invocation).
* **Batch**: `src/archi3d/orchestrator/batch.py` — implement the run creation, selection & filtering, manifest build, and `generations.csv` upserts.
* **DB utilities** (new): `src/archi3d/db/generations.py` — helpers for job identity, upserts, and normalization (thin wrapper around Phase-0 I/O utilities).

Use Phase-0 **PathResolver** accessors and atomic I/O helpers only; do **not** re-implement I/O primitives. 

---

## Functional Requirements

### A) CLI Behavior

Add/confirm subcommand:

```
archi3d batch create
  [--run-id <string>]
  [--algos <comma-separated>]
  [--image-policy <policy>]
  [--limit <int>]
  [--include <glob-or-regex>]
  [--exclude <glob-or-regex>]
  [--with-gt-only]
  [--dry-run]
```

* **Defaults**

  * `run-id`: autogenerated as UTC timestamp slug, e.g. `2025-10-18T10-22-31Z`.
  * `algos`: if omitted, use a sensible default list from adapters config (or a single default if your repo already has one). If that discovery is not available, **require** `--algos`.
  * `image-policy`: `"use_up_to_6"` (see C).
  * `limit`: unlimited.
  * `include`/`exclude`: no filtering.
  * `with-gt-only`: off (generations do **not** require GT; metrics phases do).
  * `dry-run`: off.

* **Console output**: concise summary (counts + output paths). Details go to log.

### B) Inputs

* Read **`tables/items.csv`** from Phase 1 (fail with clear error if missing). 
* Validate **Phase-0 workspace**; ensure `tables/`, `runs/`, `logs/` exist (`ensure_mutable_tree()`). 

### C) Image Selection Policy

Implement policy `"use_up_to_6"`:

* Use the **first `n_images`** as already ordered in `items.csv` (Phase 1 guarantees deterministic selection and max 6).
* If `n_images < 1` → **skip** with reason `no_images`.
* Store both the **source** image set (copied from `items.csv`) and the **used** image set for this job (equal under this policy).

Future policies may be added later; design the code to be extensible (strategy enum).

### D) Filtering Rules

Apply filters in this order:

1. `include` (match on `product_id`, `variant`, or `product_name` — implement contains/regex/glob as you prefer; document the behavior).
2. `exclude` (same fields; items that match are removed).
3. `with-gt-only` (require `gt_object_path` non-empty).
4. `n_images >= 1`.
5. `limit` (apply last).

Each excluded/skipped item must have a **reason** captured for reporting.

### E) Job Identity & Determinism

* **`image_set_hash`** = `SHA1` of the **used** image relative paths joined with `\n`.
* **`job_id`** = first 12 hex chars of `SHA1(product_id|variant|algo|image_set_hash)`.
* **`run_id`** = CLI argument or autogenerated slug (see A).
* These identities must be **stable** across re-runs with the same inputs.

### F) `tables/generations.csv` — Upsert Contract

Use Phase-0 `update_csv_atomic(path, df_new, key_cols=["run_id","job_id"])` to upsert rows. 

#### Columns to write in Phase 2 (in this order)

**Carry-over from parent (observability):**

1. `product_id`
2. `variant`
3. `manufacturer`
4. `product_name`
5. `category_l1`
6. `category_l2`
7. `category_l3`
8. `description`
9. `source_n_images`
10. `source_image_1_path` … `source_image_6_path`
11. `gt_object_path`

**Batch/job metadata:**

12. `run_id`
13. `job_id`
14. `algo`
15. `algo_version` (empty; reserved for adapters to fill later)
16. `used_n_images`
17. `used_image_1_path` … `used_image_6_path`
18. `image_set_hash`
19. `status` (set to **`enqueued`** for newly created jobs; if a matching `(run_id, job_id)` already exists, keep existing `status` unless explicit `--force` is added in the future)
20. `created_at` (ISO8601 UTC)
21. `notes` (skip reason empty, or text if enqueued with warnings)

> **Do not** predefine future metric columns here; later phases will extend the schema. Phase-0 upsert guarantees stable column order while appending new columns as they appear. 

### G) Per-Run Manifest

Write `runs/<run_id>/manifest.csv` (derived from the just-upserted rows for this run):

Required columns:

* `job_id, product_id, variant, algo, used_n_images, used_image_1_path … used_image_6_path, image_set_hash`

Optional convenience columns:

* `gt_object_path, product_name, manufacturer`

Manifest **must** contain only jobs with `status="enqueued"` for this run.

### H) Logging & Summary

Append a JSON summary line to `logs/batch_create.log` (Phase-0 helper):

```json
{
  "event": "batch_create",
  "timestamp": "...",
  "run_id": "...",
  "algos": ["..."],
  "image_policy": "use_up_to_6",
  "candidates": <int>,
  "enqueued": <int>,
  "skipped": <int>,
  "skip_reasons": { "no_images": <int>, "filtered": <int>, "with_gt_only": <int>, "duplicate_job": <int> }
}
```

* `duplicate_job`: if your selection logic attempts to create a second identical `(run_id,job_id)` during the same run, count and skip it (should not happen if identity is per above; still count if it does).
* For `--dry-run`: **don’t** write CSV/manifest; **do** print and log the projected summary with `"dry_run": true`.

### I) Idempotency & Concurrency

* Running `batch create` twice with the same inputs should **not** duplicate rows; upsert must keep one row per `(run_id, job_id)`.
* All CSV writes are **atomic** and **locked** using Phase-0 utilities; no partial files on exceptions. 

### J) Path Handling

* All paths stored in CSV must be **workspace-relative** (use `PathResolver.rel_to_workspace(...)` as needed). 
* Preserve the normalized separators convention used in Phase 1.

---

## Acceptance Criteria (Definition of Done)

1. **CLI**

   * `archi3d batch create` accepts the flags listed in A.
   * `--dry-run` works (no file writes; summary printed + logged with `dry_run: true`).

2. **Generations Registry**

   * `tables/generations.csv` exists and contains one row per job with `status="enqueued"`.
   * Upsert uses key `("run_id","job_id")`, avoiding duplicates across repeated runs of the same command.

3. **Manifest**

   * `runs/<run_id>/manifest.csv` written and includes only enqueued jobs for that run with the required columns.

4. **Logging**

   * `logs/batch_create.log` appended with a structured JSON summary including counts and reason histogram.

5. **Determinism**

   * `job_id` and `image_set_hash` are stable given the same parent row, algo, and image set.

6. **Safety**

   * All writes are atomic & locked; no temp leftovers; CSV encoding is `utf-8-sig`.

---

## Minimal Tests

Create `tests/test_phase2_batch_create.py` (or `scripts/dev/phase2_selftest.py` if pytest isn’t ready).

**Test 1 — Basic run (single algo)**
Workspace with a valid `tables/items.csv` (1 item, `n_images=2`, with GT).
`archi3d batch create --algos tripo3d_v2p5 --dry-run`

* Assert console summary shows `candidates=1,enqueued=1,skipped=0`.
* No files written; log has a `dry_run: true` summary.

**Test 2 — Real write + idempotency**
Run without `--dry-run`.

* Assert `tables/generations.csv` has exactly 1 row, `status=enqueued`.
* Assert `runs/<run_id>/manifest.csv` exists with the required columns.
  Run again with same flags.
* Assert `generations.csv` row count unchanged; `updated=0, inserted=0` from upsert.

**Test 3 — Filters & with-gt-only**
Add a second item without GT and one with `n_images=0`.

* With `--with-gt-only`, assert non-GT is skipped (`with_gt_only` reason).
* Assert `no_images` is skipped for the zero-image item.
* Histogram in log matches counts.

**Test 4 — Multi-algo, job identity**
Run with `--algos tripo3d_v2p5,trellis_single`.

* Assert 2 distinct rows/jobs for the same item (different `algo`) with different `job_id`.
* `image_set_hash` identical across the two (same images).

**Test 5 — Path relativity**

* Confirm all image and GT paths in `generations.csv` are workspace-relative.

---

## Deliverables

* Modified files:

  * `src/archi3d/cli.py` (wire command)
  * `src/archi3d/orchestrator/batch.py` (core logic)
* New file:

  * `src/archi3d/db/generations.py` (job id/hash helpers; thin upsert wrappers)
* Test or self-test script as above
* CHANGELOG:

  * **feat(phase2):** add batch create with deterministic job identity, `generations.csv` upsert, per-run manifest, and structured logging.

---

## Implementation Notes

* Use `datetime.now(timezone.utc).isoformat()` for timestamps.
* Prefer `hashlib.sha1()` for `image_set_hash` and `job_id` derivation (truncate to 12 hex for readability).
* Column order in new writes should match the order listed in **F** to minimize CSV diffs later.
* The manifest is **derived** from the just-created/updated `generations.csv` for this `run_id` (query/filter instead of re-assembling from scratch).
* Keep console output short; the log line is the canonical audit record.

---

By completing Phase 2 per this spec, the project will have a stable, queryable **SSOT for generated artifacts** and a reproducible **run manifest**, enabling Phases 3–6 to update the same registry in place.
